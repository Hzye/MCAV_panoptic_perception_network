{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from utils import bbox_iou\n",
    "from dataset import DetectionDataset, Normalise, Pad, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [92.11938007161459, 102.83839236762152, 104.90335580512152]\n",
    "std = [66.09941202519124, 70.6808655565459, 75.05305001603533]\n",
    "\n",
    "## load custom dataset + transforms\n",
    "transformed_train_data = DetectionDataset(\n",
    "    label_dict=\"det_train_shortened.json\",\n",
    "    root_dir='images/',\n",
    "    classes_file=\"data/bdd100k.names\",\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors = np.array([\n",
    "            [[116,90], [156,198], [373,326]],\n",
    "            [[30, 61], [62, 45], [59,119]],\n",
    "            [[10, 13], [16, 30], [33, 23]],\n",
    "        ]),\n",
    "    transform=transforms.Compose([\n",
    "        Normalise(\n",
    "            mean=mean,\n",
    "            std=std\n",
    "        ),\n",
    "        Pad(416),\n",
    "        ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# separate into batches\n",
    "train_loader = DataLoader(\n",
    "    transformed_train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10647, 17])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    image, labels = data.values()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulate batch size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10647, 17])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.cat((labels[0], labels[0]), 0).reshape(2, -1, 17)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape to 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21294, 17])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.reshape(-1, 17)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[93]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_preds = torch.load(\"ex_tensors/yolo_layer_output_size10647.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 85])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_preds[coco_preds[:,:,4] > 0.9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1258e-04, 1.3264e-05, 2.4101e-01, 1.8465e-05, 1.6691e-05, 2.1706e-03,\n",
       "        4.3476e-05, 8.3802e-01, 1.8591e-04, 6.2785e-06, 5.8800e-05, 1.1566e-05,\n",
       "        3.5152e-06, 1.5744e-05, 2.4891e-07, 1.0322e-07, 4.5328e-06, 3.4355e-05,\n",
       "        1.3557e-05, 3.3143e-05, 1.4100e-05, 4.8677e-06, 1.0699e-05, 3.8718e-06,\n",
       "        4.3630e-06, 1.5898e-05, 2.1193e-05, 4.1297e-07, 5.3927e-06, 1.0333e-05,\n",
       "        3.0426e-06, 1.2371e-05, 3.0568e-06, 3.5038e-05, 1.8086e-06, 3.6378e-06,\n",
       "        8.2389e-06, 9.5498e-06, 1.3817e-06, 6.0767e-05, 1.4973e-06, 1.4722e-04,\n",
       "        5.9483e-06, 1.3926e-06, 1.6157e-06, 1.3894e-05, 1.3107e-05, 6.3812e-06,\n",
       "        2.6948e-05, 4.2925e-05, 2.3591e-05, 7.2650e-05, 2.0674e-05, 1.2399e-05,\n",
       "        2.1565e-05, 8.5081e-05, 2.1246e-05, 4.1816e-05, 1.6182e-04, 2.3307e-06,\n",
       "        8.8631e-06, 2.0375e-06, 8.7221e-06, 1.2796e-05, 2.2218e-06, 1.8633e-06,\n",
       "        2.5516e-05, 1.0967e-05, 1.4625e-05, 4.6067e-06, 1.0541e-05, 8.3772e-07,\n",
       "        1.9108e-05, 1.2276e-05, 1.2190e-06, 7.4940e-06, 7.8378e-07, 2.1528e-06,\n",
       "        1.0714e-06, 8.0527e-06])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_preds[coco_preds[:,:,4] > 0.9][0][-80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0123, 0.0156, 0.0123, 0.0123, 0.0123, 0.0123, 0.0283, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123,\n",
       "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0123])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = nn.Softmax(dim=-1)\n",
    "sm(coco_preds[coco_preds[:,:,4] > 0.9][0][-80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0830)\n",
      "tensor(1.0040)\n",
      "tensor(0.9999)\n",
      "tensor(1.0000)\n",
      "tensor(1.0014)\n",
      "tensor(0.9986)\n",
      "tensor(0.9994)\n",
      "tensor(1.1833)\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(torch.sum(coco_preds[coco_preds[:,:,4] > 0.9][i][-80:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6226)\n",
      "tensor(1.1889)\n",
      "tensor(2.1072)\n",
      "tensor(0.3755)\n",
      "tensor(0.3186)\n",
      "tensor(0.4990)\n",
      "tensor(0.4447)\n",
      "tensor(0.3065)\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(torch.sum(coco_preds[0,i,-80:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretend_preds = torch.load(\"ex_tensors/yolo_layer_output_size10647.pt\")[:,:,:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulate batch size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10647, 17])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = torch.cat((pretend_preds[0], pretend_preds[0]), 0).reshape(2, -1, 17)\n",
    "# prediction = prediction.reshape(-1,17)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing each loss component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4648e-07, 2.7254e-08, 9.5856e-09,  ..., 8.5092e-06, 7.8421e-08,\n",
       "         2.4701e-08]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretend_preds[:,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(355.5738)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = labels[:,:,4]*torch.log(pretend_preds[:,:,4])\n",
    "t2 = (1 - labels[:,:,4])*torch.log(1 - pretend_preds[:,:,4])\n",
    "-torch.sum(t1 + t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2275e+09)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mse = torch.square(prediction[:,:,0] - labels[:,:,0])\n",
    "y_mse = torch.square(prediction[:,:,1] - labels[:,:,1])\n",
    "torch.sum(x_mse + y_mse)/prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(250.8586)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.sum(labels[:,:,-12:]*torch.log(pretend_preds[:,:,-12:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # losses and functions\n",
    "        self.bcwell = nn.BCEWithLogitsLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # lambda constants\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_box = 10\n",
    "        self.lambda_obj = 1\n",
    "\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "        \"\"\"\n",
    "        Computes difference between prediction and label.\n",
    "        \n",
    "        Input:\n",
    "        =prediction=    Tensor of all prediction arrays of size (n_batches, 10647, 5+n_classes).\n",
    "        =label=         Tensor of all label arryays of size (n_batches, 10647, 5+n_classes).\n",
    "        \n",
    "        Output:\n",
    "        =loss=          Total loss computed for this batch.\n",
    "        \"\"\"\n",
    "        batch_size = prediction.shape[0]\n",
    "        # check objectness for identity function - 4th ix in labels and predictions\n",
    "        obj = (label[:,:,4] == 1) # I^obj_ij\n",
    "        noobj = (label[:,:,4] == 0) # I^noobj_ij\n",
    "\n",
    "        ## box loss\n",
    "        # use generic square diff loss (mse)\n",
    "        x_mse = torch.square(prediction[:,:,0] - label[:,:,0])\n",
    "        y_mse = torch.square(prediction[:,:,1] - label[:,:,1])\n",
    "        bbox_centre_loss = torch.sum(x_mse + y_mse)\n",
    "\n",
    "        w_mse = torch.square(torch.sqrt(prediction[:,:,2]) - torch.sqrt(label[:,:,2]))\n",
    "        h_mse = torch.square(torch.sqrt(prediction[:,:,3]) - torch.sqrt(label[:,:,3]))\n",
    "        bbox_dims_loss = torch.sum(w_mse + h_mse)\n",
    "\n",
    "        bbox_loss = (1/batch_size)*(bbox_centre_loss + bbox_dims_loss)\n",
    "\n",
    "        ## object loss\n",
    "        # use binary cross entropy loss\n",
    "        t1 = label[:,:,4]*torch.log(prediction[:,:,4])\n",
    "        t2 = (1 - label[:,:,4])*torch.log(1 - prediction[:,:,4])\n",
    "        obj_loss = -(1/batch_size)*torch.sum(t1 + t2)\n",
    "\n",
    "        ## no object loss\n",
    "        noobj_loss = -(1/batch_size)*torch.sum(t1 + t2)\n",
    "\n",
    "        ## class loss\n",
    "        # use cross entropy loss\n",
    "        class_loss = -torch.sum(label[:,:,-12:]*torch.log(prediction[:,:,-12:]))\n",
    "\n",
    "        loss = self.lambda_box*bbox_loss + self.lambda_obj*obj_loss + self.lambda_noobj*noobj_loss + self.lambda_class*class_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, prediction, label, anchors):\n",
    "    \"\"\"\n",
    "    Computes difference between prediction and label.\n",
    "    \n",
    "    Input:\n",
    "    =prediction=    Tensor of all prediction arrays of size (n_batches, 10647, 5+n_classes).\n",
    "    =label=         Tensor of all label arryays of size (n_batches, 10647, 5+n_classes).\n",
    "    \n",
    "    Output:\n",
    "    =loss=          Total loss computed for this batch.\n",
    "    \"\"\"\n",
    "    # check objectness for identity function - 4th ix in labels and predictions\n",
    "    obj = (label[:,:,4] == 1) # I^obj_ij\n",
    "    noobj = (label[:,:,4] == 0) # I^noobj_ij\n",
    "\n",
    "    ## box coordinate loss\n",
    "    prediction[None] = self.sigmoid(prediction[None]) # currently x,y coords\n",
    "    label[None] = torch.log((1e-16 + label[None]/anchors)) # width and height coords\n",
    "    bbox_coord_loss = self.mse(prediction[None][obj], label[None][obj])\n",
    "\n",
    "    ## object loss\n",
    "    anchors = anchors.reshape(1,3,1,1,2)\n",
    "    box_preds = torch.cat([self.sigmoid(prediction[None]), torch.exp(prediction[None])*anchors], dim=-1)\n",
    "    result = bbox_iou(box_preds[obj], label[None][obj]).detach()\n",
    "    obj_loss = self.mse(self.sigmoid(prediction[None][obj]), result*label[None][obj])\n",
    "\n",
    "    ## no object loss\n",
    "    noobj_loss = self.bcwell((prediction[None][noobj]), (label[None][noobj]))\n",
    "\n",
    "    ## class loss\n",
    "    class_loss = self.cross_entropy((prediction[None][obj]), (label[None][obj].long()))\n",
    "\n",
    "    loss = self.lambda_box*bbox_coord_loss + self.lambda_obj*obj_loss + self.lambda_noobj*noobj_loss + self.lambda_class*class_loss\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c07979281c39b40b76a816f18a9a1b4cb65f739279c2f8cf445d62be12adec4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
