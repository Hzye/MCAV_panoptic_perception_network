{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from skimage import io, transform\n",
    "\n",
    "from model import *\n",
    "from utils import predict_transform, bbox_iou\n",
    "from dataset import filter_labels, bbox_anchorbox_iou, draw_bbox, DetectionDataset, Rescale, ToTensor, Normalise\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejig Custom Dataset\n",
    "\n",
    "**Objective:** We want each label to be [x, y, w, h, obj?, c_1, c_2, ..., c_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Custom Dataset without Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir=\"images/\",\n",
    "    classes_file=\"data/bdd100k.names\"\n",
    ")\n",
    "cats = train_data[0]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.array(load_classes(\"data/bdd100k.names\"))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 507, 85])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small = torch.load(\"ex_tensors/yolo_layer_output_size507.pt\")\n",
    "small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors in labels\n",
    "\n",
    "Find which anchorbox has the largest IoU with bbox label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corners_to_centre_dims(bbox):\n",
    "    width = bbox[2]-bbox[0]\n",
    "    height = bbox[3]-bbox[1]\n",
    "    x_centre = bbox[0]+(width/2)\n",
    "    y_centre = bbox[1]+(height/2)\n",
    "    new_bbox = np.array([x_centre, y_centre, width, height])\n",
    "    return new_bbox\n",
    "\n",
    "def centre_dims_to_corners(bbox):\n",
    "    \"\"\"\n",
    "    Converts bbox attributes of form [x_centre, y_centre, width, height] to form [x1, y1, x2, y2]. \n",
    "    \n",
    "    This form is used for easily calculating 2 bbox's IoU.\n",
    "    \"\"\"\n",
    "    x_c, y_c, w, h = bbox[:,0], bbox[:,1], bbox[:,2], bbox[:,3]\n",
    "    x1, x2 = x_c-(w/2), x_c+(w/2)\n",
    "    y1, y2 = y_c-(h/2), y_c+(h/2)\n",
    "    \n",
    "    x1 = np.expand_dims(x1, 1)\n",
    "    x2 = np.expand_dims(x2, 1)\n",
    "    y1 = np.expand_dims(y1, 1)\n",
    "    y2 = np.expand_dims(y2, 1)\n",
    "\n",
    "    new_bbox = np.concatenate((x1, y1, x2, y2), axis=1)\n",
    "    return new_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 13\n",
    "classes = np.array(load_classes(\"data/bdd100k.names\"))\n",
    "n_classes = len(classes)\n",
    "anchors = np.array([\n",
    "    [[116,90], [156,198], [373,326]],\n",
    "    [[30, 61], [62, 45], [59,119]],\n",
    "    [[10, 13], [16, 30], [33, 23]],\n",
    "])\n",
    "\n",
    "grid = np.arange(1, grid_size+1)\n",
    "a,b = np.meshgrid(grid,grid)\n",
    "\n",
    "#path = \"C:/Users/henry/Github/Projects/panoptic_perception_network/\"\n",
    "img = plt.imread(\"images/0000f77c-6257be58.jpg\")\n",
    "stride_x = ((img.shape[0])//(grid_size))\n",
    "stride_y = ((img.shape[1])//(grid_size))\n",
    "\n",
    "a *= stride_x \n",
    "b *= stride_y \n",
    "a = a - stride_x/2\n",
    "b = b - stride_y/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([203.62530269, 371.218291  , 308.36107062, 233.375848  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = np.array([49.44476737704903, 254.530367, 357.805838, 487.906215])\n",
    "category = classes[2]\n",
    "width = bbox[2]-bbox[0]\n",
    "height = bbox[3]-bbox[1]\n",
    "x_centre = bbox[0]+(width/2)\n",
    "y_centre = bbox[1]+(height/2)\n",
    "new = np.array([x_centre, y_centre, width, height])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_coords = np.concatenate((b.reshape(-1,1), a.reshape(-1,1)), axis=1)\n",
    "dists = np.sum(np.square(grid_coords-new[:2]), axis=1).reshape(13,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find coord of grid cell that bbox centre belong to\n",
    "w_idx, h_idx = np.unravel_index(dists.argmin(), dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make class type label\n",
    "class_array = (classes == category).astype(int)\n",
    "class_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cell_label = np.concatenate((new, np.array([1]), class_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145.62530269, 326.218291  , 261.62530269, 416.218291  ],\n",
       "       [125.62530269, 272.218291  , 281.62530269, 470.218291  ],\n",
       "       [ 17.12530269, 208.218291  , 390.12530269, 534.218291  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_tile = np.tile(new[:2], [3,1])\n",
    "anchor_boxes = np.concatenate((bbox_tile, anchors[0]), 1)\n",
    "anchor_boxes = centre_dims_to_corners(anchor_boxes)\n",
    "anchor_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49.44476738, 254.530367  , 357.805838  , 487.906215  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14507248, 0.42921445, 0.59181916])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchorbox_ious = bbox_anchorbox_iou(bbox, anchor_boxes)\n",
    "anchorbox_idx = np.argmax(anchorbox_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# all_labels should be an array of size (grid_size x grid_size x n_anchors, 5+n_classes)\n",
    "# so if cell [0][0] in image grid includes the centre of a bbox with IoU largest with anchor box 2\n",
    "#   the bbox attr row should be slotted into all_labels[002] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init image label\n",
    "labels = np.zeros(shape=(grid_size, grid_size, 5+n_classes))\n",
    "labels[w_idx][h_idx] = obj_cell_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([203.62530269, 371.218291  , 308.36107062, 233.375848  ,\n",
       "         1.        ,   0.        ,   0.        ,   1.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[2][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([203.62530269, 371.218291  ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:,:,:2][2][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir=\"images/\",\n",
    "    classes_file=\"data/bdd100k.names\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# bbox should just be list of [x1, y1, x2, y2]\n",
    "# categories should be list of class without ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12596/4057319556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\HENRY\\GitHub\\panoptic_perception_network\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[1;31m# print(self.classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;31m# print(categories[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0mclass_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mobj_cell_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "cats = train_data[0]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    input_img, labels = data.values()\n",
    "    print(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing manual pad transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(object):\n",
    "    \"\"\"\n",
    "    Add padding to image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # make sure output size is EITHER int or tuple\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, categories, bboxes = sample[\"image\"], sample[\"categories\"], sample[\"bboxes\"]\n",
    "\n",
    "        img_w, img_h = image.shape[1], image.shape[0]\n",
    "        w, h = self.output_size\n",
    "\n",
    "        # calculate new width and height\n",
    "        new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "        new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "        resized_image = cv2.resize(image, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        canvas = np.full((self.output_size[1], self.output_size[0], 3), 128)\n",
    "\n",
    "        canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "\n",
    "        return {\"image\": canvas, \"categories\": categories, \"bboxes\": bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    ")\n",
    "## load custom dataset + transforms\n",
    "transformed_train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    "    transform=transforms.Compose([\n",
    "        Rescale(608),\n",
    "        Normalise(0.5, 0.5),\n",
    "        Pad((608,608)),\n",
    "        ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "## dataloader\n",
    "train_loader = DataLoader(\n",
    "    transformed_train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv -> Detection (Predict Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = parse_cfg('./cfg/model.cfg')\n",
    "net_info, module_list = create_modules(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = blocks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 13), (16, 30), (33, 23)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = module_list[-1][0].anchors\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dims = int(net_info[\"height\"])\n",
    "in_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = int(modules[-1][\"classes\"], in_dims, anchors, 80)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 52, 52])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = torch.load(\"ex_tensors/yolo_layer_input.pt\")\n",
    "predict_transform(prediction, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "507 = size of first det output (yolo_layer_output_size507.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Det 1 grid size:\n",
      "13.0\n",
      "Det 2 grid size:\n",
      "26.0\n",
      "Det 3 grid size:\n",
      "52.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Det 1 grid size:\")\n",
    "print(np.sqrt(507/3))\n",
    "print(\"Det 2 grid size:\")\n",
    "print(np.sqrt((2535-507)/3))\n",
    "print(\"Det 3 grid size:\")\n",
    "print(np.sqrt((10647-2535)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(cfgfile=\"cfg/model.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)     # in 1 , out 6, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)    # in 6, out 16, 5x5 kernel\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # this time define own maxpooling\n",
    "        \n",
    "        # an affine op: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)   # 5x5 is image dim\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dims except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no activation on final layer -> output\n",
    "        return x\n",
    "testnet = TestNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "for i, data in enumerate(train_loader):\n",
    "    input_img, cat, bboxes = data.values()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #plt.imshow(input_img)\n",
    "    #input_img = norm_with_padding(input_img, 608)\n",
    "    outputs = net(Variable(input_img.float()), CUDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.8364,  15.2165,  15.7572,  ..., 605.3627, 603.0162, 604.4326]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[...,0] # select first bbox attr from all bbox tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.load(\"1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs*(outputs[:,:,4] > 0.5).float().unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10647, 85])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3549.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10647/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_corner = outputs.new(outputs.shape)\n",
    "box_corner[:,:,0] = (outputs[:,:,0] - outputs[:,:,2]/2)\n",
    "box_corner[:,:,1] = (outputs[:,:,1] - outputs[:,:,3]/2)\n",
    "box_corner[:,:,2] = (outputs[:,:,0] + outputs[:,:,2]/2)\n",
    "box_corner[:,:,3] = (outputs[:,:,1] + outputs[:,:,3]/2)\n",
    "# replace in the prediction tensor\n",
    "outputs[:,:,:4] = box_corner[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred = outputs[0] # for each image in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 85])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 80])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,5:5+80].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(5),\n",
       "indices=tensor(4))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1, 2, 3, 4, 5]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf, max_conf_idx = torch.max(image_pred[:,5:5+80], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf = max_conf.float().unsqueeze(1) # max_conf (1, 1, 85)\n",
    "max_conf_score = max_conf_idx.float().unsqueeze(1) # max_conf_score (1, 1, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 7])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "image_pred = torch.cat(seq, 1)\n",
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_idx = (torch.nonzero(image_pred[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred_ = image_pred[non_zero_idx.squeeze(), :].view(-1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_conf = 0.4\n",
    "\n",
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]\n",
    "\n",
    "n_dets = image_pred_ordered.shape[0]\n",
    "for idx in range(n_dets):\n",
    "    # print(idx)\n",
    "    # print(pred)\n",
    "    try:\n",
    "        ious = bbox_iou(image_pred_ordered[idx].unsqueeze(0), image_pred_ordered[idx+1:])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "    image_pred_ordered[idx+1:] *= iou_mask\n",
    "\n",
    "    non_zero_idx = torch.nonzero(image_pred_ordered[:,4]).squeeze()\n",
    "    image_pred_ordered = image_pred_ordered[non_zero_idx].view(-1,7)\n",
    "    # print(image_pred_ordered)\n",
    "    # print(image_pred_ordered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = image_pred_ordered.new(image_pred_ordered.size(0), 1).fill_(idx)\n",
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0000, 160.9702, 157.9619, 292.7193, 235.9922,   0.9994,   0.9109,\n",
       "           2.0000],\n",
       "        [  5.0000,  19.1932, 172.2979, 118.8749, 245.4051,   0.9974,   0.9965,\n",
       "           2.0000],\n",
       "        [  5.0000, -27.2960, 272.9604, 435.2546, 322.3747,   0.9865,   1.0000,\n",
       "           2.0000],\n",
       "        [  5.0000, 364.7225, 136.0251, 377.4218, 160.3831,   0.8234,   0.9997,\n",
       "           9.0000],\n",
       "        [  5.0000, 374.9868, 135.9671, 390.3168, 160.1495,   0.5790,   0.9998,\n",
       "           9.0000]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = batch_idx, image_pred_ordered\n",
    "output = torch.cat(seq,1)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
