{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from skimage import io, transform\n",
    "\n",
    "from model import *\n",
    "from utils import predict_transform, bbox_iou\n",
    "from dataset import filter_labels, DetectionDataset, Rescale, ToTensor, Normalise\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejig Custom Dataset\n",
    "\n",
    "**Objective:** We want each label to be [x, y, w, h, obj?, c_1, c_2, ..., c_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First find out n_unique classes/categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = load_classes(\"data/bdd100k.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing manual pad transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(object):\n",
    "    \"\"\"\n",
    "    Add padding to image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # make sure output size is EITHER int or tuple\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, categories, bboxes = sample[\"image\"], sample[\"categories\"], sample[\"bboxes\"]\n",
    "\n",
    "        img_w, img_h = image.shape[1], image.shape[0]\n",
    "        w, h = self.output_size\n",
    "\n",
    "        # calculate new width and height\n",
    "        new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "        new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "        resized_image = cv2.resize(image, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        canvas = np.full((self.output_size[1], self.output_size[0], 3), 128)\n",
    "\n",
    "        canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "\n",
    "        return {\"image\": canvas, \"categories\": categories, \"bboxes\": bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    ")\n",
    "## load custom dataset + transforms\n",
    "transformed_train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    "    transform=transforms.Compose([\n",
    "        Rescale(608),\n",
    "        Normalise(0.5, 0.5),\n",
    "        Pad((608,608)),\n",
    "        ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "## dataloader\n",
    "train_loader = DataLoader(\n",
    "    transformed_train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv -> Detection (Predict Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = parse_cfg('./cfg/model.cfg')\n",
    "net_info, module_list = create_modules(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = blocks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 13), (16, 30), (33, 23)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = module_list[-1][0].anchors\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dims = int(net_info[\"height\"])\n",
    "in_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = int(modules[-1][\"classes\"])\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 52, 52])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = torch.load(\"yolo_layer_input.pt\")\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = prediction.shape[0]\n",
    "stride = in_dims // prediction.shape[2]\n",
    "grid_size = prediction.shape[2]\n",
    "bbox_attrs = 5 + n_classes\n",
    "n_anchors = len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 2704])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = prediction.view(batch_size, bbox_attrs*n_anchors, grid_size*grid_size)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2704, 255])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = prediction.transpose(1,2).contiguous()\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8112, 85])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = prediction.view(batch_size, grid_size*grid_size*n_anchors, bbox_attrs)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9090909090909091, 1.1818181818181819),\n",
       " (1.4545454545454546, 2.727272727272727),\n",
       " (3.0, 2.090909090909091)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:,:,0] = torch.sigmoid(prediction[:,:,0]) # x - first index of each bbox attr row\n",
    "prediction[:,:,1] = torch.sigmoid(prediction[:,:,1]) # y\n",
    "prediction[:,:,4] = torch.sigmoid(prediction[:,:,4]) # obj confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(grid_size)\n",
    "a,b = np.meshgrid(grid, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_offset = torch.FloatTensor(a).view(-1,1) # size (grid_h*grid_w, 1)\n",
    "y_offset = torch.FloatTensor(b).view(-1,1) # size (grid_h*grid_w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        ...,\n",
       "        [49.],\n",
       "        [50.],\n",
       "        [51.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         [ 0.,  0.],\n",
       "         ...,\n",
       "         [51., 51.],\n",
       "         [51., 51.],\n",
       "         [51., 51.]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1, n_anchors).view(-1, 2).unsqueeze(0)\n",
    "x_y_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4176, 0.1791],\n",
       "         [0.4340, 0.5981],\n",
       "         [0.4927, 0.3315],\n",
       "         ...,\n",
       "         [0.5634, 0.3973],\n",
       "         [0.4281, 0.2278],\n",
       "         [0.3619, 0.5615]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:,:,:2] += x_y_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4176,  0.1791],\n",
       "         [ 0.4340,  0.5981],\n",
       "         [ 0.4927,  0.3315],\n",
       "         ...,\n",
       "         [51.5634, 51.3973],\n",
       "         [51.4281, 51.2278],\n",
       "         [51.3619, 51.5615]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9091, 1.1818],\n",
       "        [1.4545, 2.7273],\n",
       "        [3.0000, 2.0909]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = torch.FloatTensor(anchors)\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:,:,5:5+n_classes] = torch.sigmoid((prediction[:,:,5:5+n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.5941e+00, 1.9698e+00, 8.1939e+00,  ..., 6.1947e-04,\n",
       "          9.8315e-04, 6.9588e-04],\n",
       "         [4.7735e+00, 6.5795e+00, 6.2579e+00,  ..., 2.3491e-03,\n",
       "          3.1130e-03, 1.6503e-03],\n",
       "         [5.4195e+00, 3.6469e+00, 9.8375e+01,  ..., 1.2637e-03,\n",
       "          1.7293e-03, 1.9966e-03],\n",
       "         ...,\n",
       "         [5.6720e+02, 5.6537e+02, 3.3548e+00,  ..., 3.9487e-05,\n",
       "          4.6984e-05, 2.7504e-05],\n",
       "         [5.6571e+02, 5.6351e+02, 7.4772e+00,  ..., 2.8264e-04,\n",
       "          3.1384e-04, 2.6532e-04],\n",
       "         [5.6498e+02, 5.6718e+02, 5.6728e+01,  ..., 1.7386e-04,\n",
       "          2.2903e-04, 3.1671e-04]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:,:,:4] = prediction[:,:,:4]*stride\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8112, 85])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5642e+01, 1.1840e+01, 9.4082e+01,  ..., 6.6353e-04,\n",
       "          9.0433e-04, 5.0939e-04],\n",
       "         [1.8059e+01, 1.4927e+01, 1.0423e+02,  ..., 1.8236e-04,\n",
       "          1.0421e-03, 1.3603e-03],\n",
       "         [2.1244e+01, 1.2575e+01, 3.8758e+02,  ..., 3.3734e-03,\n",
       "          6.7615e-03, 5.6956e-03],\n",
       "         ...,\n",
       "         [4.1251e+02, 4.1118e+02, 3.3548e+00,  ..., 3.9487e-05,\n",
       "          4.6984e-05, 2.7504e-05],\n",
       "         [4.1142e+02, 4.0982e+02, 7.4772e+00,  ..., 2.8264e-04,\n",
       "          3.1384e-04, 2.6532e-04],\n",
       "         [4.1090e+02, 4.1249e+02, 5.6728e+01,  ..., 1.7386e-04,\n",
       "          2.2903e-04, 3.1671e-04]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"pred.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(cfgfile=\"cfg/model.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)     # in 1 , out 6, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)    # in 6, out 16, 5x5 kernel\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # this time define own maxpooling\n",
    "        \n",
    "        # an affine op: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)   # 5x5 is image dim\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dims except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no activation on final layer -> output\n",
    "        return x\n",
    "testnet = TestNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "for i, data in enumerate(train_loader):\n",
    "    input_img, cat, bboxes = data.values()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #plt.imshow(input_img)\n",
    "    #input_img = norm_with_padding(input_img, 608)\n",
    "    outputs = net(Variable(input_img.float()), CUDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.8364,  15.2165,  15.7572,  ..., 605.3627, 603.0162, 604.4326]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[...,0] # select first bbox attr from all bbox tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.load(\"1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs*(outputs[:,:,4] > 0.5).float().unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10647, 85])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3549.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10647/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_corner = outputs.new(outputs.shape)\n",
    "box_corner[:,:,0] = (outputs[:,:,0] - outputs[:,:,2]/2)\n",
    "box_corner[:,:,1] = (outputs[:,:,1] - outputs[:,:,3]/2)\n",
    "box_corner[:,:,2] = (outputs[:,:,0] + outputs[:,:,2]/2)\n",
    "box_corner[:,:,3] = (outputs[:,:,1] + outputs[:,:,3]/2)\n",
    "# replace in the prediction tensor\n",
    "outputs[:,:,:4] = box_corner[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred = outputs[0] # for each image in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 85])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 80])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,5:5+80].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(5),\n",
       "indices=tensor(4))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1, 2, 3, 4, 5]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf, max_conf_idx = torch.max(image_pred[:,5:5+80], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf = max_conf.float().unsqueeze(1) # max_conf (1, 1, 85)\n",
    "max_conf_score = max_conf_idx.float().unsqueeze(1) # max_conf_score (1, 1, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 7])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "image_pred = torch.cat(seq, 1)\n",
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_idx = (torch.nonzero(image_pred[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred_ = image_pred[non_zero_idx.squeeze(), :].view(-1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_conf = 0.4\n",
    "\n",
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]\n",
    "\n",
    "n_dets = image_pred_ordered.shape[0]\n",
    "for idx in range(n_dets):\n",
    "    # print(idx)\n",
    "    # print(pred)\n",
    "    try:\n",
    "        ious = bbox_iou(image_pred_ordered[idx].unsqueeze(0), image_pred_ordered[idx+1:])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "    image_pred_ordered[idx+1:] *= iou_mask\n",
    "\n",
    "    non_zero_idx = torch.nonzero(image_pred_ordered[:,4]).squeeze()\n",
    "    image_pred_ordered = image_pred_ordered[non_zero_idx].view(-1,7)\n",
    "    # print(image_pred_ordered)\n",
    "    # print(image_pred_ordered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = image_pred_ordered.new(image_pred_ordered.size(0), 1).fill_(idx)\n",
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0000, 160.9702, 157.9619, 292.7193, 235.9922,   0.9994,   0.9109,\n",
       "           2.0000],\n",
       "        [  5.0000,  19.1932, 172.2979, 118.8749, 245.4051,   0.9974,   0.9965,\n",
       "           2.0000],\n",
       "        [  5.0000, -27.2960, 272.9604, 435.2546, 322.3747,   0.9865,   1.0000,\n",
       "           2.0000],\n",
       "        [  5.0000, 364.7225, 136.0251, 377.4218, 160.3831,   0.8234,   0.9997,\n",
       "           9.0000],\n",
       "        [  5.0000, 374.9868, 135.9671, 390.3168, 160.1495,   0.5790,   0.9998,\n",
       "           9.0000]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = batch_idx, image_pred_ordered\n",
    "output = torch.cat(seq,1)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
