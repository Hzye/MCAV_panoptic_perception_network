{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "from model import parse_cfg, create_modules, EmptyLayer, DetectionLayer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = parse_cfg('./cfg/model.cfg')\n",
    "create_modules(blocks)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(Net, self).__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "\n",
    "    ## Forward pass\n",
    "    # 1. calculate output\n",
    "    # 2. transform output detection feature maps so that it can be \n",
    "    #    processed easier \n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {} # cache outputs for route layer\n",
    "        #print(\"hello\")\n",
    "\n",
    "        # iterate over module_list, pass input through each module\n",
    "        # in -> [module[0]] -> [module[1]] -> ...\n",
    "        write = 0 # used as flag to inidicate if we've encountered first detection yet for yolo \n",
    "        for idx, module in enumerate(modules):\n",
    "            module_type = (module[\"type\"])\n",
    "            #print(module_type)\n",
    "\n",
    "            ## CONV and UPSAMPLE LAYERS:\n",
    "            # pass input -> conv/upsample module -> output\n",
    "            if (module_type == \"convolutional\") or (module_type == \"upsample\"):\n",
    "                x = self.module_list[idx](x) # pass in \n",
    "            \n",
    "            ## ROUTE LAYERS\n",
    "            # case 1: layer = n\n",
    "            # - output feature map from the layer n-layers backward\n",
    "            # case 2: layer = n, m\n",
    "            # - concat(feature map from n-layers back + mth layer) along depth dim\n",
    "            elif (module_type == \"route\"):\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(n) for n in layers]\n",
    "\n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - idx # refer to current layer/module idx \n",
    "\n",
    "                # for case 1\n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[idx + (layers[0])] # pull from cache n layers ago\n",
    "                \n",
    "                # for case 2\n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - idx # refer to current layer/module idx\n",
    "                    \n",
    "                    feature_map_1 = outputs[idx + layers[0]] # take feature map from n-layers back\n",
    "                    feature_map_2 = outputs[idx + layers[1]] # take feature map from mth layer     \n",
    "\n",
    "                    # concat feature maps along depth dim\n",
    "                    x = torch.cat((feature_map_1, feature_map_2), 1)\n",
    "\n",
    "            ## SHORTCUT LAYERS\n",
    "            # from = n\n",
    "            # output = (feature map from prev layer) + (feature layer from n-layers back)\n",
    "            elif (module_type == \"shortcut\"):\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[idx-1] + outputs[idx+from_]           \n",
    "\n",
    "            ## YOLO LAYERS\n",
    "            # output = conv feature map containing bbox attributes along depth of \n",
    "            #          feature map (attribute bboxes predicted are stacjed 1 by 1\n",
    "            #          along each other)\n",
    "            # so to access 3rd bbox at cell (6,9) requires index:\n",
    "            #          map[5, 6, 2*(5+C): 3*(5+C)], where C is n_classes\n",
    "            # this sucks\n",
    "            # another issue: detection happens at 3 different scales\n",
    "            #              ->dims of pred maps different\n",
    "            #              ->use predict_transform\n",
    "            elif (module_type == \"yolo\"):\n",
    "                # concat detection maps at three diff scales into one bit tensor (possible post transform)\n",
    "\n",
    "                # cannot init empty tensor, therefore:\n",
    "                # 1. delay collector init until first detection map\n",
    "                # 2. concat maps to it after subsequent detections             \n",
    "\n",
    "                anchors = self.module_list[idx][0].anchors\n",
    "                # get input dims and n_classes\n",
    "                in_dims = int(self.net_info[\"height\"])\n",
    "                n_classes = int(module[\"classes\"])\n",
    "\n",
    "                # transform\n",
    "                x = x.data\n",
    "                x = predict_transform(\n",
    "                    prediction=x, \n",
    "                    in_dims=in_dims, \n",
    "                    anchors=anchors, \n",
    "                    n_classes=n_classes,\n",
    "                    CUDA=CUDA\n",
    "                )\n",
    "                # recall write=0 means collector hasnt been initatlised\n",
    "                if not write:\n",
    "                    detections = x\n",
    "                    write = 1\n",
    "                else:\n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "            # save current output\n",
    "            outputs[idx] = x\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Foward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy input functiont o test forward pass\n",
    "def test_forward_pass():\n",
    "    img = cv2.imread(\"images/0000f77c-62c2a288.jpg\")\n",
    "    img = cv2.resize(img, (416,416)) # resize to input dims\n",
    "    reshaped_img = img[:,:,::-1].transpose((2,0,1)) # H x W x C -> C x H x W\n",
    "    reshaped_img = reshaped_img[np.newaxis,:,:,:]/255.0 # add channel at 0 for batch norm\n",
    "    reshaped_img = torch.from_numpy(reshaped_img).float() # convert to float\n",
    "    reshaped_img = Variable(reshaped_img) # convert to variable\n",
    "    return reshaped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8070e+01, 2.6456e+01, 1.0828e+02,  ..., 4.7025e-01,\n",
       "          6.3105e-01, 5.4467e-01],\n",
       "         [2.1004e+01, 2.3269e+01, 1.3810e+02,  ..., 5.0359e-01,\n",
       "          5.6842e-01, 5.2905e-01],\n",
       "         [2.5882e+01, 1.7357e+01, 2.7089e+02,  ..., 5.4827e-01,\n",
       "          5.1742e-01, 5.8414e-01],\n",
       "         ...,\n",
       "         [5.6657e+02, 5.6628e+02, 7.6416e+00,  ..., 4.9057e-01,\n",
       "          5.3440e-01, 4.1428e-01],\n",
       "         [5.6748e+02, 5.6707e+02, 1.9345e+01,  ..., 5.0483e-01,\n",
       "          5.9651e-01, 5.2278e-01],\n",
       "         [5.6637e+02, 5.6587e+02, 1.9670e+01,  ..., 4.9743e-01,\n",
       "          4.9942e-01, 6.0571e-01]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(\"cfg/model.cfg\")\n",
    "input = test_forward_pass()\n",
    "pred = model(input, torch.cuda.is_available())\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c07979281c39b40b76a816f18a9a1b4cb65f739279c2f8cf445d62be12adec4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
