{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from skimage import io, transform\n",
    "\n",
    "from model import *\n",
    "from utils import write_results, unique, bbox_iou\n",
    "from dataset import filter_labels, DetectionDataset, Rescale, ToTensor, Normalise\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejig Custom Dataset\n",
    "\n",
    "**Objective:** We want each label to be [x, y, w, h, obj?, c_1, c_2, ..., c_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First find out n_unique classes/categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(\"D:/Datasets/bdd100k/labels/det_20/det_train.json\")\n",
    "# filtered_labels = filter_labels(\"D:/Datasets/bdd100k/labels/det_20/det_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'bus', 'car', 'pedestrian', 'traffic light', 'traffic sign', 'truck', 'bicycle'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'bus', 'car', 'pedestrian', 'traffic light', \n",
    "    'traffic sign', 'truck', 'bicycle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing manual pad transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(object):\n",
    "    \"\"\"\n",
    "    Add padding to image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # make sure output size is EITHER int or tuple\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, categories, bboxes = sample[\"image\"], sample[\"categories\"], sample[\"bboxes\"]\n",
    "\n",
    "        img_w, img_h = image.shape[1], image.shape[0]\n",
    "        w, h = self.output_size\n",
    "\n",
    "        # calculate new width and height\n",
    "        new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "        new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "        resized_image = cv2.resize(image, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        canvas = np.full((self.output_size[1], self.output_size[0], 3), 128)\n",
    "\n",
    "        canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "\n",
    "        return {\"image\": canvas, \"categories\": categories, \"bboxes\": bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    ")\n",
    "## load custom dataset + transforms\n",
    "transformed_train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    "    transform=transforms.Compose([\n",
    "        Rescale(608),\n",
    "        Normalise(0.5, 0.5),\n",
    "        Pad((608,608)),\n",
    "        ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "## dataloader\n",
    "train_loader = DataLoader(\n",
    "    transformed_train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(cfgfile=\"cfg/model.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)     # in 1 , out 6, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)    # in 6, out 16, 5x5 kernel\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # this time define own maxpooling\n",
    "        \n",
    "        # an affine op: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)   # 5x5 is image dim\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dims except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no activation on final layer -> output\n",
    "        return x\n",
    "testnet = TestNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "for i, data in enumerate(train_loader):\n",
    "    input_img, cat, bboxes = data.values()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #plt.imshow(input_img)\n",
    "    #input_img = norm_with_padding(input_img, 608)\n",
    "    outputs = net(Variable(input_img.float()), CUDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.8364,  15.2165,  15.7572,  ..., 605.3627, 603.0162, 604.4326]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[...,0] # select first bbox attr from all bbox tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.load(\"1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs*(outputs[:,:,4] > 0.5).float().unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10647, 85])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_corner = outputs.new(outputs.shape)\n",
    "box_corner[:,:,0] = (outputs[:,:,0] - outputs[:,:,2]/2)\n",
    "box_corner[:,:,1] = (outputs[:,:,1] - outputs[:,:,3]/2)\n",
    "box_corner[:,:,2] = (outputs[:,:,0] + outputs[:,:,2]/2)\n",
    "box_corner[:,:,3] = (outputs[:,:,1] + outputs[:,:,3]/2)\n",
    "# replace in the prediction tensor\n",
    "outputs[:,:,:4] = box_corner[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred = outputs[0] # for each image in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 85])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 80])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,5:5+80].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(5),\n",
       "indices=tensor(4))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1, 2, 3, 4, 5]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf, max_conf_idx = torch.max(image_pred[:,5:5+80], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf = max_conf.float().unsqueeze(1) # max_conf (1, 1, 85)\n",
    "max_conf_score = max_conf_idx.float().unsqueeze(1) # max_conf_score (1, 1, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 7])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "image_pred = torch.cat(seq, 1)\n",
    "image_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_idx = (torch.nonzero(image_pred[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred_ = image_pred[non_zero_idx.squeeze(), :].view(-1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_conf = 0.4\n",
    "\n",
    "conf_sort_idx = torch.sort(image_pred_[:,4], descending=True)[1]\n",
    "image_pred_ordered = image_pred_[conf_sort_idx]\n",
    "\n",
    "n_dets = image_pred_ordered.shape[0]\n",
    "for idx in range(n_dets):\n",
    "    # print(idx)\n",
    "    # print(pred)\n",
    "    try:\n",
    "        ious = bbox_iou(image_pred_ordered[idx].unsqueeze(0), image_pred_ordered[idx+1:])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "    image_pred_ordered[idx+1:] *= iou_mask\n",
    "\n",
    "    non_zero_idx = torch.nonzero(image_pred_ordered[:,4]).squeeze()\n",
    "    image_pred_ordered = image_pred_ordered[non_zero_idx].view(-1,7)\n",
    "    # print(image_pred_ordered)\n",
    "    # print(image_pred_ordered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.8275, 275.9133, 422.6099, 319.4225,   0.9276,   0.9997,   2.0000]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = image_pred_ordered.new(image_pred_ordered.size(0), 1).fill_(idx)\n",
    "batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0000, 160.9702, 157.9619, 292.7193, 235.9922,   0.9994,   0.9109,\n",
       "           2.0000],\n",
       "        [  5.0000,  19.1932, 172.2979, 118.8749, 245.4051,   0.9974,   0.9965,\n",
       "           2.0000],\n",
       "        [  5.0000, -27.2960, 272.9604, 435.2546, 322.3747,   0.9865,   1.0000,\n",
       "           2.0000],\n",
       "        [  5.0000, 364.7225, 136.0251, 377.4218, 160.3831,   0.8234,   0.9997,\n",
       "           9.0000],\n",
       "        [  5.0000, 374.9868, 135.9671, 390.3168, 160.1495,   0.5790,   0.9998,\n",
       "           9.0000]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = batch_idx, image_pred_ordered\n",
    "output = torch.cat(seq,1)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
