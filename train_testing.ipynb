{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from skimage import io, transform\n",
    "\n",
    "from model import *\n",
    "from dataset import filter_labels, DetectionDataset, Rescale, ToTensor, Normalise\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pad(object):\n",
    "    \"\"\"\n",
    "    Add padding to image\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # make sure output size is EITHER int or tuple\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, categories, bboxes = sample[\"image\"], sample[\"categories\"], sample[\"bboxes\"]\n",
    "\n",
    "        img_w, img_h = image.shape[1], image.shape[0]\n",
    "        w, h = self.output_size\n",
    "\n",
    "        # calculate new width and height\n",
    "        new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "        new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "        resized_image = cv2.resize(image, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        canvas = np.full((self.output_size[1], self.output_size[0], 3), 128)\n",
    "\n",
    "        canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "\n",
    "        return {\"image\": canvas, \"categories\": categories, \"bboxes\": bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels = filter_labels(\"det_train_shortened.json\")\n",
    "\n",
    "train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    ")\n",
    "## load custom dataset + transforms\n",
    "transformed_train_data = DetectionDataset(\n",
    "    label_dict=filtered_labels,\n",
    "    root_dir='images/',\n",
    "    transform=transforms.Compose([\n",
    "        Rescale(608),\n",
    "        Normalise(0.5, 0.5),\n",
    "        Pad((608,608)),\n",
    "        ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "## dataloader\n",
    "train_loader = DataLoader(\n",
    "    transformed_train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['traffic light',\n",
       " 'traffic light',\n",
       " 'traffic sign',\n",
       " 'traffic sign',\n",
       " 'car',\n",
       " 'car',\n",
       " 'traffic sign']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_data[0][\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['traffic sign',\n",
       " 'traffic sign',\n",
       " 'traffic sign',\n",
       " 'pedestrian',\n",
       " 'pedestrian',\n",
       " 'pedestrian']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_data[1][\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(cfgfile=\"cfg/model.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)     # in 1 , out 6, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)    # in 6, out 16, 5x5 kernel\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # this time define own maxpooling\n",
    "        \n",
    "        # an affine op: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)   # 5x5 is image dim\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dims except batch dim\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no activation on final layer -> output\n",
    "        return x\n",
    "testnet = TestNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "for i, data in enumerate(train_loader):\n",
    "    input_img, cat, bboxes = data.values()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #plt.imshow(input_img)\n",
    "    #input_img = norm_with_padding(input_img, 608)\n",
    "    outputs = net(Variable(input_img.float()), CUDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 20.6303,  20.2820,  21.7008,  ..., 603.5667, 603.3599, 603.8293]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[...,0] # select first bbox attr from all bbox tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
